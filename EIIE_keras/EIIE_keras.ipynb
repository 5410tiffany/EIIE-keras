{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 13 16:05:25 2020\n",
    "\n",
    "@author: Joukey\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('D:/EIIE_keras/')\n",
    "\n",
    "from pgportfolio.marketdata.datamatrices import DataMatrices\n",
    "from pgportfolio.tools.configprocess import load_config\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess = tf.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\EIIE_keras\\database\\Data.db\n",
      "1435680000 1498838400.0 1800 109 30 5e-05 poloniex 11 31 3 0.08 False False False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\EIIE_keras\\pgportfolio\\marketdata\\datamatrices.py:55: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  features=type_list)\n",
      "D:\\EIIE_keras\\pgportfolio\\marketdata\\globaldatamatrix.py:119: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  panel = panel_fillna(panel, \"both\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is so shit\n"
     ]
    }
   ],
   "source": [
    "#%% load config & data\n",
    "\n",
    "config_2 = load_config(2)\n",
    "data = DataMatrices.create_from_config(config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f785696b82eb>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f785696b82eb>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Joukey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C415EBEB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C416053BC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001C41681EB48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "#%% network structure\n",
    "\n",
    "feature_number = config_2[\"input\"][\"feature_number\"]\n",
    "rows = config_2[\"input\"][\"coin_number\"]\n",
    "columns = config_2[\"input\"][\"window_size\"]\n",
    "\n",
    "input_tensor = tf.placeholder(tf.float32, shape=[None, feature_number, rows, columns]) #[None, 3, 11, 31]\n",
    "previous_w = tf.placeholder(tf.float32, shape=[None, rows]) #[None, 11]\n",
    "input_num = tf.placeholder(tf.int32, shape=[])\n",
    "y = tf.placeholder(tf.float32, shape=[None, feature_number, rows])\n",
    "\n",
    "def allint(l):\n",
    "    return [int(i) for i in l]\n",
    "\n",
    "def build_network(layer_config, input_tensor, previous_w, input_num, y):\n",
    "    layer = layer_config['layers']\n",
    "    \n",
    "\n",
    "    \n",
    "    network = tf.transpose(input_tensor, [0, 2, 3, 1])\n",
    "    network = network / network[:, :, -1, 0, None, None]\n",
    "    network = tf.layers.conv2d(network, \n",
    "                               filters = int(layer[0][\"filter_number\"]),\n",
    "                               kernel_size = allint(layer[0][\"filter_shape\"]),\n",
    "                               strides = allint(layer[0][\"strides\"]),\n",
    "                               padding = layer[0][\"padding\"],\n",
    "                               activation = layer[0][\"activation_function\"],\n",
    "                               kernel_regularizer = layer[0][\"regularizer\"],\n",
    "                               # weight_decay=layer[0][\"weight_decay\"],\n",
    "                               )\n",
    "    \n",
    "    width = network.get_shape()[2]\n",
    "    network = tf.layers.conv2d(network, \n",
    "                               filters = int(layer[1][\"filter_number\"]),\n",
    "                               kernel_size = [1, width],\n",
    "                               strides = [1, 1],\n",
    "                               padding = \"valid\",\n",
    "                               activation = layer[1][\"activation_function\"],\n",
    "                               # kernel_regularizer=layer[1][\"regularizer\"],\n",
    "                               kernel_regularizer='l2',\n",
    "                               # weight_decay=layer[1][\"weight_decay\"],\n",
    "                               )\n",
    "    \n",
    "    width = network.get_shape()[2]\n",
    "    height = network.get_shape()[1]\n",
    "    features = network.get_shape()[3]\n",
    "    network = tf.reshape(network, [input_num, int(height), 1, int(width*features)])\n",
    "    w = tf.reshape(previous_w, [-1, int(height), 1, 1])\n",
    "    network = tf.concat([network, w], axis=3)\n",
    "    network = tf.layers.conv2d(network, filters = 1, \n",
    "                               kernel_size = [1, 1], \n",
    "                               padding=\"valid\",\n",
    "                               # kernel_regularizer = layer[2][\"regularizer\"],\n",
    "                               kernel_regularizer = 'l2'\n",
    "                               # weight_decay=layer[2][\"weight_decay\"],\n",
    "                               )\n",
    "    network = network[:, :, 0, 0]\n",
    "    btc_bias = tf.get_variable(\"btc_bias\", [1, 1], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "    btc_bias = tf.tile(btc_bias, [input_num, 1])\n",
    "    network = tf.concat([btc_bias, network], 1)\n",
    "    voting = network\n",
    "    output = tf.nn.softmax(network)\n",
    "    return output\n",
    "    \n",
    "output = build_network(config_2, input_tensor, previous_w, input_num, y)\n",
    "future_price = tf.concat([tf.ones([input_num, 1]), y[:, 0, :]], 1)\n",
    "future_omega = (future_price * output)/tf.reduce_sum(future_price * output, axis=1)[:, None]\n",
    "\n",
    "commission_ratio = config_2[\"trading\"][\"trading_consumption\"]\n",
    "\n",
    "w_t = future_omega[:input_num-1]  # rebalanced\n",
    "w_t1 = output[1:input_num]\n",
    "mu = 1 - tf.reduce_sum(tf.abs(w_t1[:, 1:]-w_t[:, 1:]), axis=1)*commission_ratio\n",
    "\n",
    "pv_vector = tf.reduce_sum(output * future_price, reduction_indices=[1])*(tf.concat([tf.ones(1), mu], axis=0))\n",
    "log_mean_free = tf.reduce_mean(tf.log(tf.reduce_sum(output * future_price, reduction_indices=[1])))\n",
    "portfolio_value = tf.reduce_prod(pv_vector)\n",
    "mean = tf.reduce_mean(pv_vector)\n",
    "log_mean = tf.reduce_mean(tf.log(pv_vector))\n",
    "standard_deviation = tf.sqrt(tf.reduce_mean((pv_vector - mean) ** 2))\n",
    "\n",
    "sharp_ratio = (mean - 1) / standard_deviation\n",
    "loss = -tf.reduce_mean(tf.log(pv_vector))\n",
    "\n",
    "# regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "# if regularization_losses:\n",
    "#     for regularization_loss in regularization_losses:\n",
    "#         loss += regularization_loss\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = config_2[\"training\"][\"learning_rate\"]\n",
    "decay_steps = config_2[\"training\"][\"decay_steps\"]\n",
    "decay_rate = config_2[\"training\"][\"decay_rate\"]\n",
    "learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# tf.reset_default_graph()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound in test is 1.3944850246036507e+30\n",
      "hi~ 0\n",
      "average time for data accessing is 1.4994144439697265e-06\n",
      "average time for training is 0.0008523170948028564\n",
      "==============================\n",
      "step 0\n",
      "------------------------------\n",
      "training loss is [-2.3442582e-05]\n",
      "\n",
      "the portfolio value on test set is 1.9160615\n",
      "log_mean is 0.00023424672\n",
      "loss_value is -0.000234\n",
      "log mean without commission fee is 0.000260\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 0 steps, whose test portfolio value is 1.9160615\n",
      "hi~ 1000\n",
      "average time for data accessing is 0.0014606683254241944\n",
      "average time for training is 0.004001790761947632\n",
      "==============================\n",
      "step 1000\n",
      "------------------------------\n",
      "training loss is [-3.0979885e-05]\n",
      "\n",
      "the portfolio value on test set is 3.2272246\n",
      "log_mean is 0.00042205345\n",
      "loss_value is -0.000422\n",
      "log mean without commission fee is 0.000557\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 1000 steps, whose test portfolio value is 3.2272246\n",
      "hi~ 2000\n",
      "average time for data accessing is 0.001424750566482544\n",
      "average time for training is 0.0038971829414367674\n",
      "==============================\n",
      "step 2000\n",
      "------------------------------\n",
      "training loss is [-3.820934e-05]\n",
      "\n",
      "the portfolio value on test set is 4.185157\n",
      "log_mean is 0.0005156868\n",
      "loss_value is -0.000516\n",
      "log mean without commission fee is 0.000707\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 2000 steps, whose test portfolio value is 4.185157\n",
      "hi~ 3000\n",
      "average time for data accessing is 0.0013836305141448975\n",
      "average time for training is 0.0038962969779968264\n",
      "==============================\n",
      "step 3000\n",
      "------------------------------\n",
      "training loss is [-6.0255683e-05]\n",
      "\n",
      "the portfolio value on test set is 10.653269\n",
      "log_mean is 0.0008522574\n",
      "loss_value is -0.000852\n",
      "log mean without commission fee is 0.001248\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 3000 steps, whose test portfolio value is 10.653269\n",
      "hi~ 4000\n",
      "average time for data accessing is 0.0014503588676452637\n",
      "average time for training is 0.003986595630645752\n",
      "==============================\n",
      "step 4000\n",
      "------------------------------\n",
      "training loss is [-7.264096e-05]\n",
      "\n",
      "the portfolio value on test set is 19.65052\n",
      "log_mean is 0.0010728039\n",
      "loss_value is -0.001073\n",
      "log mean without commission fee is 0.001658\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 4000 steps, whose test portfolio value is 19.65052\n",
      "hi~ 5000\n",
      "average time for data accessing is 0.0014428551197052002\n",
      "average time for training is 0.0040646114349365235\n",
      "==============================\n",
      "step 5000\n",
      "------------------------------\n",
      "training loss is [-8.6928725e-05]\n",
      "\n",
      "the portfolio value on test set is 36.11211\n",
      "log_mean is 0.0012920127\n",
      "loss_value is -0.001292\n",
      "log mean without commission fee is 0.002154\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 5000 steps, whose test portfolio value is 36.11211\n",
      "hi~ 6000\n",
      "average time for data accessing is 0.0015181689262390137\n",
      "average time for training is 0.004084314107894897\n",
      "==============================\n",
      "step 6000\n",
      "------------------------------\n",
      "training loss is [-9.366791e-05]\n",
      "\n",
      "the portfolio value on test set is 46.859886\n",
      "log_mean is 0.0013858642\n",
      "loss_value is -0.001386\n",
      "log mean without commission fee is 0.002414\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 6000 steps, whose test portfolio value is 46.859886\n",
      "hi~ 7000\n",
      "average time for data accessing is 0.0015009241104125977\n",
      "average time for training is 0.003936530590057373\n",
      "==============================\n",
      "step 7000\n",
      "------------------------------\n",
      "training loss is [-9.69154e-05]\n",
      "\n",
      "the portfolio value on test set is 54.361748\n",
      "log_mean is 0.0014393596\n",
      "loss_value is -0.001439\n",
      "log mean without commission fee is 0.002563\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 7000 steps, whose test portfolio value is 54.361748\n",
      "hi~ 8000\n",
      "average time for data accessing is 0.0013560349941253661\n",
      "average time for training is 0.003901386260986328\n",
      "==============================\n",
      "step 8000\n",
      "------------------------------\n",
      "training loss is [-9.9563906e-05]\n",
      "\n",
      "the portfolio value on test set is 59.924393\n",
      "log_mean is 0.0014744543\n",
      "loss_value is -0.001474\n",
      "log mean without commission fee is 0.002673\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 8000 steps, whose test portfolio value is 59.924393\n",
      "hi~ 9000\n",
      "average time for data accessing is 0.0014625823497772218\n",
      "average time for training is 0.003979873657226562\n",
      "==============================\n",
      "step 9000\n",
      "------------------------------\n",
      "training loss is [-0.00010088855]\n",
      "\n",
      "the portfolio value on test set is 64.58606\n",
      "log_mean is 0.0015014405\n",
      "loss_value is -0.001501\n",
      "log mean without commission fee is 0.002752\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 9000 steps, whose test portfolio value is 64.58606\n",
      "hi~ 10000\n",
      "average time for data accessing is 0.0014275767803192138\n",
      "average time for training is 0.003985372066497803\n",
      "==============================\n",
      "step 10000\n",
      "------------------------------\n",
      "training loss is [-0.00010265306]\n",
      "\n",
      "the portfolio value on test set is 73.126976\n",
      "log_mean is 0.0015461806\n",
      "loss_value is -0.001546\n",
      "log mean without commission fee is 0.002869\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 10000 steps, whose test portfolio value is 73.126976\n",
      "hi~ 11000\n",
      "average time for data accessing is 0.0013252787590026856\n",
      "average time for training is 0.003968150615692139\n",
      "==============================\n",
      "step 11000\n",
      "------------------------------\n",
      "training loss is [-0.00010407714]\n",
      "\n",
      "the portfolio value on test set is 81.933815\n",
      "log_mean is 0.0015871444\n",
      "loss_value is -0.001587\n",
      "log mean without commission fee is 0.002992\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 11000 steps, whose test portfolio value is 81.933815\n",
      "hi~ 12000\n",
      "average time for data accessing is 0.0013276865482330321\n",
      "average time for training is 0.0040252537727355955\n",
      "==============================\n",
      "step 12000\n",
      "------------------------------\n",
      "training loss is [-0.000105281266]\n",
      "\n",
      "the portfolio value on test set is 89.85979\n",
      "log_mean is 0.001620406\n",
      "loss_value is -0.001620\n",
      "log mean without commission fee is 0.003071\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 12000 steps, whose test portfolio value is 89.85979\n",
      "hi~ 13000\n",
      "average time for data accessing is 0.0014531617164611817\n",
      "average time for training is 0.003977290391921997\n",
      "==============================\n",
      "step 13000\n",
      "------------------------------\n",
      "training loss is [-0.000105966305]\n",
      "\n",
      "the portfolio value on test set is 90.04746\n",
      "log_mean is 0.0016211584\n",
      "loss_value is -0.001621\n",
      "log mean without commission fee is 0.003072\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 13000 steps, whose test portfolio value is 90.04746\n",
      "hi~ 14000\n",
      "average time for data accessing is 0.0013390400409698487\n",
      "average time for training is 0.0038998806476593017\n",
      "==============================\n",
      "step 14000\n",
      "------------------------------\n",
      "training loss is [-0.00010677227]\n",
      "\n",
      "the portfolio value on test set is 95.38412\n",
      "log_mean is 0.0016418985\n",
      "loss_value is -0.001642\n",
      "log mean without commission fee is 0.003127\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 14000 steps, whose test portfolio value is 95.38412\n",
      "hi~ 15000\n",
      "average time for data accessing is 0.0014490525722503662\n",
      "average time for training is 0.003950895071029663\n",
      "==============================\n",
      "step 15000\n",
      "------------------------------\n",
      "training loss is [-0.00010775618]\n",
      "\n",
      "the portfolio value on test set is 101.51834\n",
      "log_mean is 0.0016643517\n",
      "loss_value is -0.001664\n",
      "log mean without commission fee is 0.003209\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 15000 steps, whose test portfolio value is 101.51834\n",
      "hi~ 16000\n",
      "average time for data accessing is 0.0014724037647247314\n",
      "average time for training is 0.00398405385017395\n",
      "==============================\n",
      "step 16000\n",
      "------------------------------\n",
      "training loss is [-0.0001085264]\n",
      "\n",
      "the portfolio value on test set is 105.09025\n",
      "log_mean is 0.001676809\n",
      "loss_value is -0.001677\n",
      "log mean without commission fee is 0.003252\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 16000 steps, whose test portfolio value is 105.09025\n",
      "hi~ 17000\n",
      "average time for data accessing is 0.001332658290863037\n",
      "average time for training is 0.003991275548934936\n",
      "==============================\n",
      "step 17000\n",
      "------------------------------\n",
      "training loss is [-0.00010742921]\n",
      "\n",
      "the portfolio value on test set is 111.93971\n",
      "log_mean is 0.0016995531\n",
      "loss_value is -0.001700\n",
      "log mean without commission fee is 0.003312\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 17000 steps, whose test portfolio value is 111.93971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi~ 18000\n",
      "average time for data accessing is 0.001487295389175415\n",
      "average time for training is 0.004119687080383301\n",
      "==============================\n",
      "step 18000\n",
      "------------------------------\n",
      "training loss is [-0.000118450145]\n",
      "\n",
      "the portfolio value on test set is 112.238106\n",
      "log_mean is 0.0017005126\n",
      "loss_value is -0.001701\n",
      "log mean without commission fee is 0.003331\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 18000 steps, whose test portfolio value is 112.238106\n",
      "hi~ 19000\n",
      "average time for data accessing is 0.0015154640674591064\n",
      "average time for training is 0.004163532495498657\n",
      "==============================\n",
      "step 19000\n",
      "------------------------------\n",
      "training loss is [-0.000118180615]\n",
      "\n",
      "the portfolio value on test set is 107.70729\n",
      "log_mean is 0.0016856685\n",
      "loss_value is -0.001686\n",
      "log mean without commission fee is 0.003299\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 20000\n",
      "average time for data accessing is 0.0014720189571380616\n",
      "average time for training is 0.004037446737289428\n",
      "==============================\n",
      "step 20000\n",
      "------------------------------\n",
      "training loss is [-0.000118818345]\n",
      "\n",
      "the portfolio value on test set is 112.041885\n",
      "log_mean is 0.0016998821\n",
      "loss_value is -0.001700\n",
      "log mean without commission fee is 0.003358\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 21000\n",
      "average time for data accessing is 0.001464966058731079\n",
      "average time for training is 0.004046499490737915\n",
      "==============================\n",
      "step 21000\n",
      "------------------------------\n",
      "training loss is [-0.00011260349]\n",
      "\n",
      "the portfolio value on test set is 111.550064\n",
      "log_mean is 0.0016982966\n",
      "loss_value is -0.001698\n",
      "log mean without commission fee is 0.003398\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 22000\n",
      "average time for data accessing is 0.001498483896255493\n",
      "average time for training is 0.004089498043060303\n",
      "==============================\n",
      "step 22000\n",
      "------------------------------\n",
      "training loss is [-0.00012405521]\n",
      "\n",
      "the portfolio value on test set is 100.10133\n",
      "log_mean is 0.0016592878\n",
      "loss_value is -0.001659\n",
      "log mean without commission fee is 0.003403\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 23000\n",
      "average time for data accessing is 0.001403045654296875\n",
      "average time for training is 0.004028407573699952\n",
      "==============================\n",
      "step 23000\n",
      "------------------------------\n",
      "training loss is [-0.00012353716]\n",
      "\n",
      "the portfolio value on test set is 101.960976\n",
      "log_mean is 0.0016659183\n",
      "loss_value is -0.001666\n",
      "log mean without commission fee is 0.003413\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 24000\n",
      "average time for data accessing is 0.001510897159576416\n",
      "average time for training is 0.004055522441864014\n",
      "==============================\n",
      "step 24000\n",
      "------------------------------\n",
      "training loss is [-0.00011972479]\n",
      "\n",
      "the portfolio value on test set is 103.85346\n",
      "log_mean is 0.0016725422\n",
      "loss_value is -0.001673\n",
      "log mean without commission fee is 0.003442\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 25000\n",
      "average time for data accessing is 0.0014597656726837158\n",
      "average time for training is 0.003991690874099732\n",
      "==============================\n",
      "step 25000\n",
      "------------------------------\n",
      "training loss is [-0.00012056073]\n",
      "\n",
      "the portfolio value on test set is 104.86085\n",
      "log_mean is 0.0016760218\n",
      "loss_value is -0.001676\n",
      "log mean without commission fee is 0.003479\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 26000\n",
      "average time for data accessing is 0.0014583792686462403\n",
      "average time for training is 0.003994096040725708\n",
      "==============================\n",
      "step 26000\n",
      "------------------------------\n",
      "training loss is [-0.00012388846]\n",
      "\n",
      "the portfolio value on test set is 101.3338\n",
      "log_mean is 0.0016636953\n",
      "loss_value is -0.001664\n",
      "log mean without commission fee is 0.003479\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 27000\n",
      "average time for data accessing is 0.0014438540935516357\n",
      "average time for training is 0.003976097583770752\n",
      "==============================\n",
      "step 27000\n",
      "------------------------------\n",
      "training loss is [-0.00012428296]\n",
      "\n",
      "the portfolio value on test set is 104.05254\n",
      "log_mean is 0.0016732328\n",
      "loss_value is -0.001673\n",
      "log mean without commission fee is 0.003516\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 28000\n",
      "average time for data accessing is 0.001374703884124756\n",
      "average time for training is 0.00398874044418335\n",
      "==============================\n",
      "step 28000\n",
      "------------------------------\n",
      "training loss is [-0.000121195204]\n",
      "\n",
      "the portfolio value on test set is 108.16792\n",
      "log_mean is 0.0016872077\n",
      "loss_value is -0.001687\n",
      "log mean without commission fee is 0.003553\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 29000\n",
      "average time for data accessing is 0.0013889310359954834\n",
      "average time for training is 0.003946005582809448\n",
      "==============================\n",
      "step 29000\n",
      "------------------------------\n",
      "training loss is [-0.00012223203]\n",
      "\n",
      "the portfolio value on test set is 109.189186\n",
      "log_mean is 0.0016905922\n",
      "loss_value is -0.001691\n",
      "log mean without commission fee is 0.003590\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 30000\n",
      "average time for data accessing is 0.0014631481170654297\n",
      "average time for training is 0.00397680926322937\n",
      "==============================\n",
      "step 30000\n",
      "------------------------------\n",
      "training loss is [-0.0001229917]\n",
      "\n",
      "the portfolio value on test set is 113.86135\n",
      "log_mean is 0.0017056843\n",
      "loss_value is -0.001706\n",
      "log mean without commission fee is 0.003624\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 30000 steps, whose test portfolio value is 113.86135\n",
      "hi~ 31000\n",
      "average time for data accessing is 0.0015033104419708253\n",
      "average time for training is 0.003983652830123902\n",
      "==============================\n",
      "step 31000\n",
      "------------------------------\n",
      "training loss is [-0.00012365375]\n",
      "\n",
      "the portfolio value on test set is 116.01166\n",
      "log_mean is 0.0017124244\n",
      "loss_value is -0.001712\n",
      "log mean without commission fee is 0.003651\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 31000 steps, whose test portfolio value is 116.01166\n",
      "hi~ 32000\n",
      "average time for data accessing is 0.0014879031181335449\n",
      "average time for training is 0.004012064695358276\n",
      "==============================\n",
      "step 32000\n",
      "------------------------------\n",
      "training loss is [-0.00012393272]\n",
      "\n",
      "the portfolio value on test set is 117.45446\n",
      "log_mean is 0.0017168763\n",
      "loss_value is -0.001717\n",
      "log mean without commission fee is 0.003670\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 32000 steps, whose test portfolio value is 117.45446\n",
      "hi~ 33000\n",
      "average time for data accessing is 0.0015092182159423828\n",
      "average time for training is 0.003996209859848022\n",
      "==============================\n",
      "step 33000\n",
      "------------------------------\n",
      "training loss is [-0.00012437937]\n",
      "\n",
      "the portfolio value on test set is 116.58746\n",
      "log_mean is 0.0017142076\n",
      "loss_value is -0.001714\n",
      "log mean without commission fee is 0.003670\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 34000\n",
      "average time for data accessing is 0.0014988131523132324\n",
      "average time for training is 0.0039551455974578854\n",
      "==============================\n",
      "step 34000\n",
      "------------------------------\n",
      "training loss is [-0.00012493474]\n",
      "\n",
      "the portfolio value on test set is 123.552734\n",
      "log_mean is 0.0017351101\n",
      "loss_value is -0.001735\n",
      "log mean without commission fee is 0.003713\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 34000 steps, whose test portfolio value is 123.552734\n",
      "hi~ 35000\n",
      "average time for data accessing is 0.0014035415649414063\n",
      "average time for training is 0.0039593977928161625\n",
      "==============================\n",
      "step 35000\n",
      "------------------------------\n",
      "training loss is [-0.00013852252]\n",
      "\n",
      "the portfolio value on test set is 125.72077\n",
      "log_mean is 0.0017413764\n",
      "loss_value is -0.001741\n",
      "log mean without commission fee is 0.003713\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 35000 steps, whose test portfolio value is 125.72077\n",
      "hi~ 36000\n",
      "average time for data accessing is 0.0014904005527496338\n",
      "average time for training is 0.0041332228183746334\n",
      "==============================\n",
      "step 36000\n",
      "------------------------------\n",
      "training loss is [-0.0001425901]\n",
      "\n",
      "the portfolio value on test set is 131.96098\n",
      "log_mean is 0.0017588271\n",
      "loss_value is -0.001759\n",
      "log mean without commission fee is 0.003749\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 36000 steps, whose test portfolio value is 131.96098\n",
      "hi~ 37000\n",
      "average time for data accessing is 0.0014860661029815674\n",
      "average time for training is 0.0040063982009887695\n",
      "==============================\n",
      "step 37000\n",
      "------------------------------\n",
      "training loss is [-0.00014137213]\n",
      "\n",
      "the portfolio value on test set is 134.3822\n",
      "log_mean is 0.0017653784\n",
      "loss_value is -0.001765\n",
      "log mean without commission fee is 0.003767\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 37000 steps, whose test portfolio value is 134.3822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi~ 38000\n",
      "average time for data accessing is 0.001425569772720337\n",
      "average time for training is 0.0039484605789184574\n",
      "==============================\n",
      "step 38000\n",
      "------------------------------\n",
      "training loss is [-0.00014403413]\n",
      "\n",
      "the portfolio value on test set is 134.68246\n",
      "log_mean is 0.0017661814\n",
      "loss_value is -0.001766\n",
      "log mean without commission fee is 0.003773\n",
      "\n",
      "==============================\n",
      "\n",
      "get better model at 38000 steps, whose test portfolio value is 134.68246\n",
      "hi~ 39000\n",
      "average time for data accessing is 0.0014941909313201905\n",
      "average time for training is 0.004040781736373901\n",
      "==============================\n",
      "step 39000\n",
      "------------------------------\n",
      "training loss is [-0.00013555506]\n",
      "\n",
      "the portfolio value on test set is 134.6653\n",
      "log_mean is 0.0017661358\n",
      "loss_value is -0.001766\n",
      "log mean without commission fee is 0.003796\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 40000\n",
      "average time for data accessing is 0.0014544613361358644\n",
      "average time for training is 0.004009496450424194\n",
      "==============================\n",
      "step 40000\n",
      "------------------------------\n",
      "training loss is [-0.00014402071]\n",
      "\n",
      "the portfolio value on test set is 127.30019\n",
      "log_mean is 0.0017458749\n",
      "loss_value is -0.001746\n",
      "log mean without commission fee is 0.003761\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 41000\n",
      "average time for data accessing is 0.0015068511962890625\n",
      "average time for training is 0.004022620677947998\n",
      "==============================\n",
      "step 41000\n",
      "------------------------------\n",
      "training loss is [-0.00014417966]\n",
      "\n",
      "the portfolio value on test set is 130.32297\n",
      "log_mean is 0.0017543286\n",
      "loss_value is -0.001754\n",
      "log mean without commission fee is 0.003780\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 42000\n",
      "average time for data accessing is 0.001502915382385254\n",
      "average time for training is 0.003993548631668091\n",
      "==============================\n",
      "step 42000\n",
      "------------------------------\n",
      "training loss is [-0.00014075745]\n",
      "\n",
      "the portfolio value on test set is 127.53137\n",
      "log_mean is 0.0017465291\n",
      "loss_value is -0.001747\n",
      "log mean without commission fee is 0.003786\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 43000\n",
      "average time for data accessing is 0.0014830691814422608\n",
      "average time for training is 0.003969388484954834\n",
      "==============================\n",
      "step 43000\n",
      "------------------------------\n",
      "training loss is [-0.00014826738]\n",
      "\n",
      "the portfolio value on test set is 121.4296\n",
      "log_mean is 0.0017288676\n",
      "loss_value is -0.001729\n",
      "log mean without commission fee is 0.003785\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 44000\n",
      "average time for data accessing is 0.0014714789390563965\n",
      "average time for training is 0.0038744604587554934\n",
      "==============================\n",
      "step 44000\n",
      "------------------------------\n",
      "training loss is [-0.00014889993]\n",
      "\n",
      "the portfolio value on test set is 121.9894\n",
      "log_mean is 0.0017305233\n",
      "loss_value is -0.001731\n",
      "log mean without commission fee is 0.003805\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 45000\n",
      "average time for data accessing is 0.0014983248710632325\n",
      "average time for training is 0.00394913125038147\n",
      "==============================\n",
      "step 45000\n",
      "------------------------------\n",
      "training loss is [-0.00014992914]\n",
      "\n",
      "the portfolio value on test set is 122.51579\n",
      "log_mean is 0.0017320742\n",
      "loss_value is -0.001732\n",
      "log mean without commission fee is 0.003816\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 46000\n",
      "average time for data accessing is 0.0013810222148895263\n",
      "average time for training is 0.0038358938694000246\n",
      "==============================\n",
      "step 46000\n",
      "------------------------------\n",
      "training loss is [-0.00015099495]\n",
      "\n",
      "the portfolio value on test set is 118.26364\n",
      "log_mean is 0.0017193502\n",
      "loss_value is -0.001719\n",
      "log mean without commission fee is 0.003822\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 47000\n",
      "average time for data accessing is 0.00150946044921875\n",
      "average time for training is 0.003913493394851685\n",
      "==============================\n",
      "step 47000\n",
      "------------------------------\n",
      "training loss is [-0.00015436811]\n",
      "\n",
      "the portfolio value on test set is 112.93482\n",
      "log_mean is 0.0017027417\n",
      "loss_value is -0.001703\n",
      "log mean without commission fee is 0.003796\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 48000\n",
      "average time for data accessing is 0.0013672523498535155\n",
      "average time for training is 0.003752647399902344\n",
      "==============================\n",
      "step 48000\n",
      "------------------------------\n",
      "training loss is [-0.00015544222]\n",
      "\n",
      "the portfolio value on test set is 100.41155\n",
      "log_mean is 0.0016604022\n",
      "loss_value is -0.001660\n",
      "log mean without commission fee is 0.003765\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 49000\n",
      "average time for data accessing is 0.001426516056060791\n",
      "average time for training is 0.003799900770187378\n",
      "==============================\n",
      "step 49000\n",
      "------------------------------\n",
      "training loss is [-0.00015155073]\n",
      "\n",
      "the portfolio value on test set is 84.06793\n",
      "log_mean is 0.0015964075\n",
      "loss_value is -0.001596\n",
      "log mean without commission fee is 0.003723\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 50000\n",
      "average time for data accessing is 0.001456500768661499\n",
      "average time for training is 0.0038944389820098875\n",
      "==============================\n",
      "step 50000\n",
      "------------------------------\n",
      "training loss is [-0.0001563657]\n",
      "\n",
      "the portfolio value on test set is 77.96164\n",
      "log_mean is 0.0015692437\n",
      "loss_value is -0.001569\n",
      "log mean without commission fee is 0.003752\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 51000\n",
      "average time for data accessing is 0.0014775447845458984\n",
      "average time for training is 0.003925904750823974\n",
      "==============================\n",
      "step 51000\n",
      "------------------------------\n",
      "training loss is [-0.00015684236]\n",
      "\n",
      "the portfolio value on test set is 78.53418\n",
      "log_mean is 0.0015718784\n",
      "loss_value is -0.001572\n",
      "log mean without commission fee is 0.003797\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 52000\n",
      "average time for data accessing is 0.0014230248928070069\n",
      "average time for training is 0.003972923755645752\n",
      "==============================\n",
      "step 52000\n",
      "------------------------------\n",
      "training loss is [-0.00016334734]\n",
      "\n",
      "the portfolio value on test set is 74.33065\n",
      "log_mean is 0.0015520615\n",
      "loss_value is -0.001552\n",
      "log mean without commission fee is 0.003809\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 53000\n",
      "average time for data accessing is 0.0014559507369995117\n",
      "average time for training is 0.00406201982498169\n",
      "==============================\n",
      "step 53000\n",
      "------------------------------\n",
      "training loss is [-0.00016379612]\n",
      "\n",
      "the portfolio value on test set is 71.11462\n",
      "log_mean is 0.0015361287\n",
      "loss_value is -0.001536\n",
      "log mean without commission fee is 0.003811\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 54000\n",
      "average time for data accessing is 0.001410637378692627\n",
      "average time for training is 0.003992310285568237\n",
      "==============================\n",
      "step 54000\n",
      "------------------------------\n",
      "training loss is [-0.0001658291]\n",
      "\n",
      "the portfolio value on test set is 71.98986\n",
      "log_mean is 0.0015405358\n",
      "loss_value is -0.001541\n",
      "log mean without commission fee is 0.003837\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 55000\n",
      "average time for data accessing is 0.001443948268890381\n",
      "average time for training is 0.003953999519348145\n",
      "==============================\n",
      "step 55000\n",
      "------------------------------\n",
      "training loss is [-0.00016761506]\n",
      "\n",
      "the portfolio value on test set is 74.830185\n",
      "log_mean is 0.001554474\n",
      "loss_value is -0.001554\n",
      "log mean without commission fee is 0.003872\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 56000\n",
      "average time for data accessing is 0.0013847212791442871\n",
      "average time for training is 0.003962715148925781\n",
      "==============================\n",
      "step 56000\n",
      "------------------------------\n",
      "training loss is [-0.00016656172]\n",
      "\n",
      "the portfolio value on test set is 73.4601\n",
      "log_mean is 0.0015478185\n",
      "loss_value is -0.001548\n",
      "log mean without commission fee is 0.003890\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 57000\n",
      "average time for data accessing is 0.0014989938735961914\n",
      "average time for training is 0.0039554636478424075\n",
      "==============================\n",
      "step 57000\n",
      "------------------------------\n",
      "training loss is [-0.00016715967]\n",
      "\n",
      "the portfolio value on test set is 75.06108\n",
      "log_mean is 0.0015555841\n",
      "loss_value is -0.001556\n",
      "log mean without commission fee is 0.003924\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 58000\n",
      "average time for data accessing is 0.0014735403060913085\n",
      "average time for training is 0.003999919176101685\n",
      "==============================\n",
      "step 58000\n",
      "------------------------------\n",
      "training loss is [-0.00017031017]\n",
      "\n",
      "the portfolio value on test set is 74.82471\n",
      "log_mean is 0.0015544483\n",
      "loss_value is -0.001554\n",
      "log mean without commission fee is 0.003957\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi~ 59000\n",
      "average time for data accessing is 0.001526843786239624\n",
      "average time for training is 0.004054634809494018\n",
      "==============================\n",
      "step 59000\n",
      "------------------------------\n",
      "training loss is [-0.00016928886]\n",
      "\n",
      "the portfolio value on test set is 75.5433\n",
      "log_mean is 0.0015578916\n",
      "loss_value is -0.001558\n",
      "log mean without commission fee is 0.003974\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 60000\n",
      "average time for data accessing is 0.0015588691234588623\n",
      "average time for training is 0.004103624343872071\n",
      "==============================\n",
      "step 60000\n",
      "------------------------------\n",
      "training loss is [-0.00017048554]\n",
      "\n",
      "the portfolio value on test set is 69.87894\n",
      "log_mean is 0.0015298143\n",
      "loss_value is -0.001530\n",
      "log mean without commission fee is 0.003961\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 61000\n",
      "average time for data accessing is 0.001519916296005249\n",
      "average time for training is 0.003973548650741577\n",
      "==============================\n",
      "step 61000\n",
      "------------------------------\n",
      "training loss is [-0.00017136711]\n",
      "\n",
      "the portfolio value on test set is 73.42212\n",
      "log_mean is 0.0015476315\n",
      "loss_value is -0.001548\n",
      "log mean without commission fee is 0.004000\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 62000\n",
      "average time for data accessing is 0.0015151081085205077\n",
      "average time for training is 0.004068872690200806\n",
      "==============================\n",
      "step 62000\n",
      "------------------------------\n",
      "training loss is [-0.00017157457]\n",
      "\n",
      "the portfolio value on test set is 72.394875\n",
      "log_mean is 0.001542556\n",
      "loss_value is -0.001543\n",
      "log mean without commission fee is 0.004014\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 63000\n",
      "average time for data accessing is 0.0015205817222595215\n",
      "average time for training is 0.004122408151626587\n",
      "==============================\n",
      "step 63000\n",
      "------------------------------\n",
      "training loss is [-0.0001718336]\n",
      "\n",
      "the portfolio value on test set is 68.12988\n",
      "log_mean is 0.0015206839\n",
      "loss_value is -0.001521\n",
      "log mean without commission fee is 0.004011\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 64000\n",
      "average time for data accessing is 0.0014315285682678222\n",
      "average time for training is 0.003936413526535034\n",
      "==============================\n",
      "step 64000\n",
      "------------------------------\n",
      "training loss is [-0.00017178984]\n",
      "\n",
      "the portfolio value on test set is 70.58562\n",
      "log_mean is 0.0015334383\n",
      "loss_value is -0.001533\n",
      "log mean without commission fee is 0.004042\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 65000\n",
      "average time for data accessing is 0.0015201365947723389\n",
      "average time for training is 0.004159360885620117\n",
      "==============================\n",
      "step 65000\n",
      "------------------------------\n",
      "training loss is [-0.00017130474]\n",
      "\n",
      "the portfolio value on test set is 67.40673\n",
      "log_mean is 0.0015168393\n",
      "loss_value is -0.001517\n",
      "log mean without commission fee is 0.004048\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 66000\n",
      "average time for data accessing is 0.0016049585342407227\n",
      "average time for training is 0.004249570846557617\n",
      "==============================\n",
      "step 66000\n",
      "------------------------------\n",
      "training loss is [-0.00017321963]\n",
      "\n",
      "the portfolio value on test set is 75.96429\n",
      "log_mean is 0.001559893\n",
      "loss_value is -0.001560\n",
      "log mean without commission fee is 0.004093\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 67000\n",
      "average time for data accessing is 0.0014716167449951173\n",
      "average time for training is 0.004038348436355591\n",
      "==============================\n",
      "step 67000\n",
      "------------------------------\n",
      "training loss is [-0.00017378425]\n",
      "\n",
      "the portfolio value on test set is 74.65018\n",
      "log_mean is 0.0015536067\n",
      "loss_value is -0.001554\n",
      "log mean without commission fee is 0.004094\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 68000\n",
      "average time for data accessing is 0.0014692435264587402\n",
      "average time for training is 0.003968712568283081\n",
      "==============================\n",
      "step 68000\n",
      "------------------------------\n",
      "training loss is [-0.00017357465]\n",
      "\n",
      "the portfolio value on test set is 71.387276\n",
      "log_mean is 0.0015375065\n",
      "loss_value is -0.001538\n",
      "log mean without commission fee is 0.004103\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 69000\n",
      "average time for data accessing is 0.0015082190036773682\n",
      "average time for training is 0.003989246368408203\n",
      "==============================\n",
      "step 69000\n",
      "------------------------------\n",
      "training loss is [-0.00017260943]\n",
      "\n",
      "the portfolio value on test set is 74.2077\n",
      "log_mean is 0.0015514655\n",
      "loss_value is -0.001551\n",
      "log mean without commission fee is 0.004136\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 70000\n",
      "average time for data accessing is 0.0014700629711151122\n",
      "average time for training is 0.003940886497497559\n",
      "==============================\n",
      "step 70000\n",
      "------------------------------\n",
      "training loss is [-0.00017088736]\n",
      "\n",
      "the portfolio value on test set is 68.58676\n",
      "log_mean is 0.0015230908\n",
      "loss_value is -0.001523\n",
      "log mean without commission fee is 0.004128\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 71000\n",
      "average time for data accessing is 0.0015215089321136475\n",
      "average time for training is 0.0040754742622375485\n",
      "==============================\n",
      "step 71000\n",
      "------------------------------\n",
      "training loss is [-0.00017433625]\n",
      "\n",
      "the portfolio value on test set is 76.463425\n",
      "log_mean is 0.001562252\n",
      "loss_value is -0.001562\n",
      "log mean without commission fee is 0.004162\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 72000\n",
      "average time for data accessing is 0.0015150690078735351\n",
      "average time for training is 0.004118921756744384\n",
      "==============================\n",
      "step 72000\n",
      "------------------------------\n",
      "training loss is [-0.00017447905]\n",
      "\n",
      "the portfolio value on test set is 77.63171\n",
      "log_mean is 0.0015677157\n",
      "loss_value is -0.001568\n",
      "log mean without commission fee is 0.004175\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 73000\n",
      "average time for data accessing is 0.0015370838642120362\n",
      "average time for training is 0.00407039999961853\n",
      "==============================\n",
      "step 73000\n",
      "------------------------------\n",
      "training loss is [-0.00017394571]\n",
      "\n",
      "the portfolio value on test set is 72.019615\n",
      "log_mean is 0.0015406837\n",
      "loss_value is -0.001541\n",
      "log mean without commission fee is 0.004180\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 74000\n",
      "average time for data accessing is 0.0013609323501586913\n",
      "average time for training is 0.0040470163822174075\n",
      "==============================\n",
      "step 74000\n",
      "------------------------------\n",
      "training loss is [-0.0001742194]\n",
      "\n",
      "the portfolio value on test set is 76.89975\n",
      "log_mean is 0.0015643014\n",
      "loss_value is -0.001564\n",
      "log mean without commission fee is 0.004208\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 75000\n",
      "average time for data accessing is 0.0014557478427886963\n",
      "average time for training is 0.004016212463378906\n",
      "==============================\n",
      "step 75000\n",
      "------------------------------\n",
      "training loss is [-0.00017551339]\n",
      "\n",
      "the portfolio value on test set is 78.539764\n",
      "log_mean is 0.0015719034\n",
      "loss_value is -0.001572\n",
      "log mean without commission fee is 0.004223\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 76000\n",
      "average time for data accessing is 0.0014794645309448241\n",
      "average time for training is 0.003985994815826416\n",
      "==============================\n",
      "step 76000\n",
      "------------------------------\n",
      "training loss is [-0.0001744025]\n",
      "\n",
      "the portfolio value on test set is 74.15813\n",
      "log_mean is 0.0015512242\n",
      "loss_value is -0.001551\n",
      "log mean without commission fee is 0.004210\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 77000\n",
      "average time for data accessing is 0.0014492640495300294\n",
      "average time for training is 0.003988191366195679\n",
      "==============================\n",
      "step 77000\n",
      "------------------------------\n",
      "training loss is [-0.00017521298]\n",
      "\n",
      "the portfolio value on test set is 75.74043\n",
      "log_mean is 0.0015588299\n",
      "loss_value is -0.001559\n",
      "log mean without commission fee is 0.004216\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 78000\n",
      "average time for data accessing is 0.0014480621814727784\n",
      "average time for training is 0.003923380613327026\n",
      "==============================\n",
      "step 78000\n",
      "------------------------------\n",
      "training loss is [-0.00017398578]\n",
      "\n",
      "the portfolio value on test set is 73.80117\n",
      "log_mean is 0.0015494877\n",
      "loss_value is -0.001549\n",
      "log mean without commission fee is 0.004228\n",
      "\n",
      "==============================\n",
      "\n",
      "hi~ 79000\n",
      "average time for data accessing is 0.0014882841110229492\n",
      "average time for training is 0.004004676580429077\n",
      "==============================\n",
      "step 79000\n",
      "------------------------------\n",
      "training loss is [-0.000174174]\n",
      "\n",
      "the portfolio value on test set is 72.323135\n",
      "log_mean is 0.0015421987\n",
      "loss_value is -0.001542\n",
      "log mean without commission fee is 0.004240\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the portfolio value train No.2 is 71.714745 log_mean is 0.0015391565, the training time is 449 seconds\n"
     ]
    }
   ],
   "source": [
    "#%% training & logging & testing network\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "\n",
    "train_dir = \"train_package\"\n",
    "s_path = \"./\" + train_dir + \"/\" + '2'+ \"/netfile\"\n",
    "l_path = \"./\" + train_dir + \"/\" + '2' + \"/tensorboard\"\n",
    "\n",
    "console_level = logging.INFO\n",
    "logfile_level = logging.DEBUG\n",
    "\n",
    "logging.basicConfig(filename=l_path.replace(\"tensorboard\",\"programlog\"), level=console_level)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(console_level)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "\n",
    "test_set = data.get_test_set()\n",
    "train_set = data.get_training_set()\n",
    "array = np.maximum.reduce(test_set['y'][:, 0, :], 1)\n",
    "total = 1.0\n",
    "for i in array:\n",
    "    total = total * i\n",
    "\n",
    "# logging.info(\"upper bound in test is %s\" % total)\n",
    "print(\"upper bound in test is %s\" % total)\n",
    "            \n",
    "# init_tensor_board(log_file_dir) # tensorboard related func. omit now\n",
    "starttime = time.time()\n",
    "\n",
    "total_data_time = 0\n",
    "total_training_time = 0\n",
    "best_metric = 0\n",
    "for i in range(config_2['training'][\"steps\"]):\n",
    "    step_start = time.time()\n",
    "    \n",
    "    batch = data.next_batch()\n",
    "    batch_x = batch[\"X\"]\n",
    "    batch_y = batch[\"y\"]\n",
    "    batch_last_w = batch[\"last_w\"]\n",
    "    batch_setw = batch[\"setw\"]\n",
    "\n",
    "    finish_data = time.time()\n",
    "    total_data_time += (finish_data - step_start)\n",
    "    # self._agent.train(x, y, last_w=last_w, setw=setw)\n",
    "    \n",
    "    tflearn.is_training(True, sess)\n",
    "    \n",
    "    results = sess.run([train_step, output], feed_dict={input_tensor: batch_x, y: batch_y, previous_w: batch_last_w, input_num: batch_x.shape[0]})\n",
    "    batch_setw(results[-1][:, 1:])\n",
    "    \n",
    "    total_training_time += time.time() - finish_data\n",
    "    if i % 1000 == 0:\n",
    "        print('hi~', i)\n",
    "        # logging.info(\"average time for data accessing is %s\"%(total_data_time/1000))\n",
    "        # logging.info(\"average time for training is %s\"%(total_training_time/1000))\n",
    "        print(\"average time for data accessing is %s\"%(total_data_time/1000))\n",
    "        print(\"average time for training is %s\"%(total_training_time/1000))\n",
    "        total_training_time = 0\n",
    "        total_data_time = 0\n",
    "        # self.log_between_steps(i)\n",
    "        tflearn.is_training(False, sess)\n",
    "        \n",
    "        batch_x = test_set[\"X\"]\n",
    "        batch_y = test_set[\"y\"]\n",
    "        batch_last_w = test_set[\"last_w\"]\n",
    "        batch_setw = test_set[\"setw\"]\n",
    "        \n",
    "        v_pv, v_log_mean, v_loss, v_log_mean_free, weights = \\\n",
    "        sess.run([portfolio_value, log_mean, loss, log_mean_free, output], feed_dict={input_tensor: batch_x, y: batch_y, previous_w: batch_last_w, input_num: batch_x.shape[0]})\n",
    "        \n",
    "        # logging.info('='*30)\n",
    "        # logging.info('step %d' % i)\n",
    "        # logging.info('-'*30)\n",
    "        print('='*30)\n",
    "        print('step %d' % i)\n",
    "        print('-'*30)\n",
    "        \n",
    "        batch_x = train_set[\"X\"]\n",
    "        batch_y = train_set[\"y\"]\n",
    "        batch_last_w = train_set[\"last_w\"]\n",
    "        batch_setw = train_set[\"setw\"]\n",
    "        \n",
    "        loss_value = sess.run([loss], feed_dict={input_tensor: batch_x, y: batch_y, previous_w: batch_last_w, input_num: batch_x.shape[0]})\n",
    "        # logging.info('training loss is %s\\n' % loss_value)\n",
    "        # logging.info('the portfolio value on test set is %s\\nlog_mean is %s\\n'\n",
    "        #              'loss_value is %3f\\nlog mean without commission fee is %3f\\n' % \\\n",
    "        #              (v_pv, v_log_mean, v_loss, v_log_mean_free))\n",
    "        # logging.info('='*30+\"\\n\")\n",
    "        \n",
    "        print('training loss is %s\\n' % loss_value)\n",
    "        print('the portfolio value on test set is %s\\nlog_mean is %s\\n'\n",
    "                     'loss_value is %3f\\nlog mean without commission fee is %3f\\n' % \\\n",
    "                     (v_pv, v_log_mean, v_loss, v_log_mean_free))\n",
    "        print('='*30+\"\\n\")\n",
    "        \n",
    "        if v_pv > best_metric:\n",
    "            best_metric = v_pv\n",
    "            # logging.info(\"get better model at %s steps,\"\n",
    "            #              \" whose test portfolio value is %s\" % (i, v_pv))\n",
    "            print(\"get better model at %s steps,\"\n",
    "                         \" whose test portfolio value is %s\" % (i, v_pv))\n",
    "\n",
    "batch_x = test_set[\"X\"]\n",
    "batch_y = test_set[\"y\"]\n",
    "batch_last_w = test_set[\"last_w\"]\n",
    "batch_setw = test_set[\"setw\"]\n",
    "\n",
    "pv, log_mean = sess.run([portfolio_value, log_mean], feed_dict={input_tensor: batch_x, y: batch_y, previous_w: batch_last_w, input_num: batch_x.shape[0]})\n",
    "\n",
    "# logging.warning('the portfolio value train No.%s is %s log_mean is %s,'\n",
    "#                 ' the training time is %d seconds' % ('2', pv, log_mean, time.time() - starttime))\n",
    "print('the portfolio value train No.%s is %s log_mean is %s,'\n",
    "                ' the training time is %d seconds' % ('2', pv, log_mean, time.time() - starttime))\n",
    "\n",
    "# return self.__log_result_csv(index, time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.275093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48.46767\n",
    "35.595684\n",
    "38.275093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2776, 3, 11, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
